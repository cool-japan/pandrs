use criterion::{black_box, criterion_group, criterion_main, Criterion, Throughput};
use pandrs::error::Result;
use pandrs::optimized::jit::*;
use pandrs::optimized::OptimizedDataFrame;
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use std::fs;
use std::path::Path;
use std::time::Instant;

/// Performance baseline data structure
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PerformanceBaseline {
    pub version: String,
    pub timestamp: u64,
    pub benchmarks: HashMap<String, BenchmarkResult>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BenchmarkResult {
    pub mean_time_ns: f64,
    pub std_dev_ns: f64,
    pub throughput_ops_per_sec: Option<f64>,
    pub memory_usage_bytes: Option<usize>,
}

/// Performance regression detector
pub struct RegressionDetector {
    baseline_path: String,
    current_baseline: Option<PerformanceBaseline>,
    regression_threshold: f64, // 10% = 0.1
}

impl RegressionDetector {
    pub fn new(baseline_path: &str, regression_threshold: f64) -> Self {
        let current_baseline = Self::load_baseline(baseline_path);
        Self {
            baseline_path: baseline_path.to_string(),
            current_baseline,
            regression_threshold,
        }
    }

    fn load_baseline(path: &str) -> Option<PerformanceBaseline> {
        if Path::new(path).exists() {
            let content = fs::read_to_string(path).ok()?;
            serde_json::from_str(&content).ok()
        } else {
            None
        }
    }

    pub fn save_baseline(&self, baseline: &PerformanceBaseline) -> Result<()> {
        let json = serde_json::to_string_pretty(baseline)
            .map_err(|e| pandrs::error::Error::IoError(format!("JSON serialization error: {}", e)))?;
        fs::write(&self.baseline_path, json)
            .map_err(|e| pandrs::error::Error::IoError(format!("Failed to write baseline: {}", e)))?;
        Ok(())
    }

    pub fn check_regression(&self, benchmark_name: &str, current_time_ns: f64) -> Option<f64> {
        if let Some(ref baseline) = self.current_baseline {
            if let Some(baseline_result) = baseline.benchmarks.get(benchmark_name) {
                let regression_ratio = (current_time_ns - baseline_result.mean_time_ns) / baseline_result.mean_time_ns;
                if regression_ratio > self.regression_threshold {
                    return Some(regression_ratio);
                }
            }
        }
        None
    }
}

/// Create standardized test data for regression testing
fn create_regression_test_dataframe(size: usize) -> Result<OptimizedDataFrame> {
    let mut df = OptimizedDataFrame::new();
    
    // Use deterministic data for consistent benchmarks
    let mut int_data = Vec::with_capacity(size);
    let mut float_data = Vec::with_capacity(size);
    let mut string_data = Vec::with_capacity(size);
    
    for i in 0..size {
        int_data.push((i % 10000) as i64);
        float_data.push((i as f64) * 0.1 + (i % 100) as f64);
        string_data.push(format!("Cat_{}", i % 100));
    }
    
    df.add_int_column("value", int_data)?;
    df.add_float_column("score", float_data)?;
    df.add_string_column("category", string_data)?;
    
    Ok(df)
}

/// Micro-benchmarks for regression detection
fn regression_dataframe_creation(c: &mut Criterion) {
    let mut group = c.benchmark_group("regression_dataframe_creation");
    let detector = RegressionDetector::new("benchmark_baseline.json", 0.1);
    
    let test_size = 50_000;
    group.throughput(Throughput::Elements(test_size as u64));
    
    group.bench_function("standard_creation", |b| {
        let start = Instant::now();
        b.iter(|| {
            black_box(create_regression_test_dataframe(test_size).unwrap())
        });
        let elapsed_ns = start.elapsed().as_nanos() as f64;
        
        if let Some(regression) = detector.check_regression("dataframe_creation", elapsed_ns) {
            eprintln!("‚ö†Ô∏è  REGRESSION DETECTED in dataframe_creation: {:.2}% slower", regression * 100.0);
        }
    });
    
    group.finish();
}

fn regression_aggregation_operations(c: &mut Criterion) {
    let mut group = c.benchmark_group("regression_aggregations");
    let detector = RegressionDetector::new("benchmark_baseline.json", 0.1);
    
    let df = create_regression_test_dataframe(100_000).unwrap();
    group.throughput(Throughput::Elements(100_000));
    
    // Test core aggregation operations
    let operations = [
        ("sum", Box::new(|df: &OptimizedDataFrame| df.sum("value").unwrap()) as Box<dyn Fn(&OptimizedDataFrame) -> f64>),
        ("mean", Box::new(|df: &OptimizedDataFrame| df.mean("score").unwrap())),
        ("min", Box::new(|df: &OptimizedDataFrame| df.min("value").unwrap())),
        ("max", Box::new(|df: &OptimizedDataFrame| df.max("value").unwrap())),
    ];
    
    for (op_name, op_func) in operations {
        group.bench_function(op_name, |b| {
            let start = Instant::now();
            b.iter(|| {
                black_box(op_func(&df))
            });
            let elapsed_ns = start.elapsed().as_nanos() as f64;
            
            if let Some(regression) = detector.check_regression(&format!("aggregation_{}", op_name), elapsed_ns) {
                eprintln!("‚ö†Ô∏è  REGRESSION DETECTED in {}: {:.2}% slower", op_name, regression * 100.0);
            }
        });
    }
    
    group.finish();
}

fn regression_groupby_operations(c: &mut Criterion) {
    let mut group = c.benchmark_group("regression_groupby");
    let detector = RegressionDetector::new("benchmark_baseline.json", 0.1);
    
    let df = create_regression_test_dataframe(50_000).unwrap();
    group.throughput(Throughput::Elements(50_000));
    
    group.bench_function("parallel_groupby", |b| {
        let start = Instant::now();
        b.iter(|| {
            black_box(df.par_groupby(&["category"]).unwrap())
        });
        let elapsed_ns = start.elapsed().as_nanos() as f64;
        
        if let Some(regression) = detector.check_regression("parallel_groupby", elapsed_ns) {
            eprintln!("‚ö†Ô∏è  REGRESSION DETECTED in parallel_groupby: {:.2}% slower", regression * 100.0);
        }
    });
    
    group.finish();
}

fn regression_simd_operations(c: &mut Criterion) {
    let mut group = c.benchmark_group("regression_simd");
    let detector = RegressionDetector::new("benchmark_baseline.json", 0.1);
    
    let test_data: Vec<f64> = (0..100_000).map(|i| i as f64 * 0.1).collect();
    group.throughput(Throughput::Elements(100_000));
    
    // Test SIMD operations
    let simd_operations = [
        ("simd_sum", Box::new(|data: &[f64]| simd_sum_f64(data)) as Box<dyn Fn(&[f64]) -> f64>),
        ("simd_mean", Box::new(|data: &[f64]| simd_mean_f64(data))),
        ("simd_min", Box::new(|data: &[f64]| simd_min_f64(data))),
        ("simd_max", Box::new(|data: &[f64]| simd_max_f64(data))),
    ];
    
    for (op_name, op_func) in simd_operations {
        group.bench_function(op_name, |b| {
            let start = Instant::now();
            b.iter(|| {
                black_box(op_func(&test_data))
            });
            let elapsed_ns = start.elapsed().as_nanos() as f64;
            
            if let Some(regression) = detector.check_regression(&format!("simd_{}", op_name), elapsed_ns) {
                eprintln!("‚ö†Ô∏è  REGRESSION DETECTED in {}: {:.2}% slower", op_name, regression * 100.0);
            }
        });
    }
    
    group.finish();
}

fn regression_io_operations(c: &mut Criterion) {
    let mut group = c.benchmark_group("regression_io");
    let detector = RegressionDetector::new("benchmark_baseline.json", 0.1);
    
    let df = create_regression_test_dataframe(10_000).unwrap();
    group.throughput(Throughput::Elements(10_000));
    
    group.bench_function("csv_write", |b| {
        let start = Instant::now();
        b.iter(|| {
            let temp_path = format!("/tmp/regression_test_{}.csv", rand::random::<u32>());
            df.to_csv(&temp_path, true).unwrap();
            std::fs::remove_file(&temp_path).ok();
            black_box(())
        });
        let elapsed_ns = start.elapsed().as_nanos() as f64;
        
        if let Some(regression) = detector.check_regression("csv_write", elapsed_ns) {
            eprintln!("‚ö†Ô∏è  REGRESSION DETECTED in csv_write: {:.2}% slower", regression * 100.0);
        }
    });
    
    #[cfg(feature = "parquet")]
    group.bench_function("parquet_write", |b| {
        let start = Instant::now();
        b.iter(|| {
            use pandrs::io::{write_parquet, ParquetCompression};
            let temp_path = format!("/tmp/regression_test_{}.parquet", rand::random::<u32>());
            write_parquet(&df, &temp_path, Some(ParquetCompression::Snappy)).unwrap();
            std::fs::remove_file(&temp_path).ok();
            black_box(())
        });
        let elapsed_ns = start.elapsed().as_nanos() as f64;
        
        if let Some(regression) = detector.check_regression("parquet_write", elapsed_ns) {
            eprintln!("‚ö†Ô∏è  REGRESSION DETECTED in parquet_write: {:.2}% slower", regression * 100.0);
        }
    });
    
    group.finish();
}

/// Performance baseline establishment
#[allow(dead_code)]
fn establish_baseline() {
    println!("üéØ Establishing performance baseline...");
    
    let detector = RegressionDetector::new("benchmark_baseline.json", 0.1);
    let mut baseline = PerformanceBaseline {
        version: "0.1.0-alpha.4".to_string(),
        timestamp: std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap()
            .as_secs(),
        benchmarks: HashMap::new(),
    };
    
    // Establish baselines for key operations
    let df = create_regression_test_dataframe(50_000).unwrap();
    let test_data: Vec<f64> = (0..50_000).map(|i| i as f64 * 0.1).collect();
    
    // Measure key operations
    let operations: &[(&str, Box<dyn Fn()>)] = &[
        ("dataframe_creation", Box::new(|| {
            create_regression_test_dataframe(50_000).unwrap();
        })),
        ("sum_operation", Box::new(|| {
            df.sum("value").unwrap();
        })),
        ("groupby_operation", Box::new(|| {
            df.par_groupby(&["category"]).unwrap();
        })),
        ("simd_sum", Box::new(|| {
            simd_sum_f64(&test_data);
        })),
    ];
    
    for (name, operation) in operations {
        let start = Instant::now();
        for _ in 0..10 {
            operation();
        }
        let elapsed = start.elapsed();
        let mean_time_ns = elapsed.as_nanos() as f64 / 10.0;
        
        baseline.benchmarks.insert(name.to_string(), BenchmarkResult {
            mean_time_ns,
            std_dev_ns: 0.0, // Would calculate from multiple runs in real implementation
            throughput_ops_per_sec: None,
            memory_usage_bytes: None,
        });
        
        println!("üìä Baseline for {}: {:.2}ms", name, mean_time_ns / 1_000_000.0);
    }
    
    detector.save_baseline(&baseline).unwrap();
    println!("‚úÖ Baseline saved to benchmark_baseline.json");
}

criterion_group!(
    regression_benches,
    regression_dataframe_creation,
    regression_aggregation_operations,
    regression_groupby_operations,
    regression_simd_operations,
    regression_io_operations
);

criterion_main!(regression_benches);

#[cfg(test)]
#[cfg(test)]
mod tests {
    #[allow(unused_imports)]
    use super::*;
    
    #[test]
    fn test_baseline_creation() {
        establish_baseline();
    }
}