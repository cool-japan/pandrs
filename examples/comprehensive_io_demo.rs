//! Comprehensive I/O Examples for PandRS Beta.2
//!
//! This example demonstrates all the enhanced I/O capabilities implemented in Beta.2:
//! 1. Excel Support Enhancement - formula preservation, cell formatting, named ranges
//! 2. Advanced Parquet Features - schema evolution, predicate pushdown, streaming
//! 3. Database Integration Expansion - async operations, connection pooling, transactions
//!
//! To run this example:
//!   cargo run --example comprehensive_io_alpha6_example --features "excel streaming sql"
//!
//! Note: This example focuses on demonstrating the API concepts and enhanced features
//! rather than being fully compilable, as it showcases planned Phase 2 Alpha.6 capabilities.

use pandrs::dataframe::base::DataFrame;
use pandrs::error::Result;
use pandrs::series::Series;

#[cfg(feature = "excel")]
use std::path::Path;

#[cfg(feature = "parquet")]
use pandrs::io::{ParquetCompression, ParquetMetadata, ParquetWriteOptions};

#[cfg(feature = "excel")]
use pandrs::io::{ExcelReadOptions, ExcelWorkbookInfo, ExcelWriteOptions, NamedRange};

#[cfg(feature = "sql")]
use pandrs::io::{
    ColumnDefinition, DatabaseConnection, InsertMethod, PoolConfig, SqlWriteOptions, TableSchema,
    WriteMode,
};

#[cfg(any(feature = "parquet", feature = "sql"))]
#[cfg(feature = "sql")]
use std::time::Duration;

#[allow(clippy::result_large_err)]
fn main() -> Result<()> {
    println!("PandRS Comprehensive I/O Capabilities - Phase 2 Alpha.6");
    println!("=======================================================");

    // Create sample datasets for demonstration
    let sample_df = create_sample_financial_dataframe()?;
    let large_df = create_large_dataset(50_000)?;

    println!("\n=== 1. Excel Support Enhancement Demonstrations ===");
    excel_enhancement_examples(&sample_df)?;

    println!("\n=== 2. Advanced Parquet Features Demonstrations ===");
    parquet_advanced_examples(&sample_df, &large_df)?;

    println!("\n=== 3. Database Integration Expansion Demonstrations ===");
    database_advanced_examples(&sample_df)?;

    println!("\n=== 4. Cross-Format Integration Examples ===");
    cross_format_integration_examples(&sample_df)?;

    println!("\n=== 5. Performance and Scalability Demonstrations ===");
    performance_scalability_examples(&large_df)?;

    println!("\nAll Phase 2 Alpha.6 I/O demonstrations completed successfully!");
    Ok(())
}

/// Comprehensive Excel Support Enhancement Examples
#[allow(clippy::result_large_err)]
fn excel_enhancement_examples(df: &DataFrame) -> Result<()> {
    println!("\n--- Excel Formula Preservation and Cell Formatting ---");

    #[cfg(not(feature = "excel"))]
    {
        // Suppress unused parameter warning when excel feature is not enabled
        let _ = df;
        println!("Excel features not enabled. Enable with --features excel");
    }

    #[cfg(feature = "excel")]
    {
        // 1. Enhanced Excel Writing with Formatting
        let write_options = ExcelWriteOptions {
            preserve_formulas: true,
            apply_formatting: true,
            write_named_ranges: true,
            protect_sheets: false,
            optimize_large_files: true,
        };

        println!("Writing Excel file with enhanced formatting...");
        write_excel_enhanced(
            "demo_formatted.xlsx",
            df,
            Some("Financial_Data"),
            &write_options,
        )?;

        // 2. Multi-sheet Excel Operations
        println!("Creating multi-sheet Excel workbook...");
        create_multi_sheet_workbook(df)?;

        // 3. Named Ranges and Formula Processing
        println!("Demonstrating named ranges and formula preservation...");
        named_ranges_example(df)?;

        // 4. Large Excel File Optimization
        println!("Testing large Excel file optimization...");
        large_excel_file_example(df)?;

        // 5. Cell Formatting Analysis
        if Path::new("demo_formatted.xlsx").exists() {
            println!("Analyzing Excel file formatting and metadata...");
            analyze_excel_file("demo_formatted.xlsx")?;
        }

        // 6. Advanced Excel Reading Options
        println!("Demonstrating advanced Excel reading capabilities...");
        advanced_excel_reading_example()?;
    }

    #[cfg(not(feature = "excel"))]
    {
        println!("Excel features require 'excel' feature flag to be enabled.");
        println!("Compile with: cargo run --example comprehensive_io_alpha6_example --features \"excel\"");
    }

    Ok(())
}

/// Advanced Parquet Features Examples
#[allow(clippy::result_large_err)]
fn parquet_advanced_examples(df: &DataFrame, large_df: &DataFrame) -> Result<()> {
    println!("\n--- Advanced Parquet Features ---");

    // 1. Schema Evolution and Migration
    println!("Demonstrating Parquet schema evolution...");
    schema_evolution_example(df)?;

    // 2. Advanced Compression Options
    #[cfg(feature = "parquet")]
    {
        println!("Testing advanced compression algorithms...");
        compression_comparison_example(df)?;
    }

    // 3. Predicate Pushdown for Efficient Reading
    println!("Demonstrating predicate pushdown optimization...");
    predicate_pushdown_example(large_df)?;

    // 4. Streaming Read/Write for Large Datasets
    #[cfg(feature = "streaming")]
    {
        println!("Testing streaming Parquet operations...");
        streaming_parquet_example(large_df)?;
    }

    // 5. Schema Analysis and Migration Planning
    println!("Analyzing Parquet schema complexity...");
    schema_analysis_example()?;

    // 6. Memory-Efficient Chunked Processing
    println!("Demonstrating chunked Parquet processing...");
    chunked_parquet_processing_example(large_df)?;

    // 7. Metadata Extraction and Statistics
    #[cfg(feature = "parquet")]
    {
        println!("Extracting comprehensive Parquet metadata...");
        parquet_metadata_analysis_example()?;
    }

    Ok(())
}

/// Database Integration Expansion Examples
#[allow(clippy::result_large_err)]
fn database_advanced_examples(df: &DataFrame) -> Result<()> {
    println!("\n--- Database Integration Expansion ---");

    // 1. Async Connection Pooling
    #[cfg(feature = "sql")]
    {
        println!("Setting up async database connection pool...");
        // Note: Async example would require async main function
        println!("  Async connection pool example would be demonstrated here");
    }

    // 2. Transaction Management
    println!("Demonstrating transaction management...");
    transaction_management_example(df)?;

    // 3. Type-Safe SQL Query Builder
    println!("Using type-safe SQL query builder...");
    query_builder_example(df)?;

    // 4. Database Schema Introspection
    #[cfg(feature = "sql")]
    {
        println!("Analyzing database schema...");
        schema_introspection_example()?;
    }

    // 5. Bulk Insert Operations
    #[cfg(feature = "sql")]
    {
        println!("Testing bulk insert operations...");
        bulk_insert_example(df)?;
    }

    // 6. Connection Statistics and Monitoring
    #[cfg(feature = "sql")]
    {
        println!("Monitoring connection pool statistics...");
        // Note: Async example would require async main function
        println!("  Connection monitoring example would be demonstrated here");
    }

    // 7. Multi-Database Support
    #[cfg(feature = "sql")]
    {
        println!("Demonstrating multi-database integration...");
        multi_database_example(df)?;
    }

    Ok(())
}

/// Cross-Format Integration Examples
#[allow(clippy::result_large_err)]
fn cross_format_integration_examples(df: &DataFrame) -> Result<()> {
    println!("\n--- Cross-Format Integration ---");

    // 1. Excel to Parquet Migration
    println!("Converting Excel to optimized Parquet format...");
    excel_to_parquet_migration(df)?;

    // 2. Database to Excel Reporting
    println!("Generating Excel reports from database queries...");
    database_to_excel_reporting(df)?;

    // 3. Parquet to Database ETL Pipeline
    println!("ETL pipeline: Parquet → Database with transformations...");
    parquet_to_database_etl(df)?;

    // 4. Format Performance Comparison
    println!("Comparing I/O performance across formats...");
    format_performance_comparison(df)?;

    Ok(())
}

/// Performance and Scalability Examples
#[allow(clippy::result_large_err)]
fn performance_scalability_examples(large_df: &DataFrame) -> Result<()> {
    println!("\n--- Performance and Scalability ---");

    // 1. Memory Usage Optimization
    println!("Testing memory-efficient I/O operations...");
    memory_optimization_example(large_df)?;

    // 2. Parallel I/O Operations
    println!("Demonstrating parallel I/O processing...");
    parallel_io_example(large_df)?;

    // 3. Large Dataset Streaming
    #[cfg(feature = "streaming")]
    {
        println!("Processing very large datasets with streaming...");
        large_dataset_streaming_example(large_df)?;
    }

    // 4. I/O Performance Benchmarking
    println!("Benchmarking I/O operations...");
    io_performance_benchmarks(large_df)?;

    Ok(())
}

// ============================================================================
// Excel Enhancement Implementation Examples
// ============================================================================

#[cfg(feature = "excel")]
#[allow(clippy::result_large_err)]
fn write_excel_enhanced<P: AsRef<Path>>(
    _path: P,
    df: &DataFrame,
    sheet_name: Option<&str>,
    options: &ExcelWriteOptions,
) -> Result<()> {
    // This would integrate with the enhanced Excel writer
    // For demonstration, we'll show the API usage
    println!(
        "  • Writing with formula preservation: {}",
        options.preserve_formulas
    );
    println!("  • Applying cell formatting: {}", options.apply_formatting);
    println!("  • Including named ranges: {}", options.write_named_ranges);
    println!(
        "  • Large file optimization: {}",
        options.optimize_large_files
    );

    // Simulate enhanced Excel writing
    let sheet_name = sheet_name.unwrap_or("Sheet1");
    println!("  • Created sheet: {}", sheet_name);
    println!("  • Applied enhanced formatting to {} rows", df.row_count());

    Ok(())
}

#[cfg(feature = "excel")]
#[allow(clippy::result_large_err)]
fn create_multi_sheet_workbook(df: &DataFrame) -> Result<()> {
    println!("  Creating workbook with multiple sheets...");

    let sheets = vec![("Summary", df), ("Detailed_Data", df), ("Analysis", df)];

    for (sheet_name, data) in sheets {
        println!(
            "    - Sheet '{}': {} rows, {} columns",
            sheet_name,
            data.row_count(),
            data.column_count()
        );
    }

    println!("  Multi-sheet workbook created with 3 sheets");
    Ok(())
}

#[cfg(feature = "excel")]
#[allow(clippy::result_large_err)]
fn named_ranges_example(_df: &DataFrame) -> Result<()> {
    let named_ranges = vec![
        NamedRange {
            name: "SalesData".to_string(),
            sheet_name: "Sheet1".to_string(),
            range: "A1:E100".to_string(),
            comment: Some("Primary sales data range".to_string()),
        },
        NamedRange {
            name: "SummaryArea".to_string(),
            sheet_name: "Sheet1".to_string(),
            range: "G1:J10".to_string(),
            comment: Some("Summary statistics area".to_string()),
        },
    ];

    println!("  Created named ranges:");
    for range in &named_ranges {
        println!(
            "    - {} ({}): {}",
            range.name, range.sheet_name, range.range
        );
    }

    // Demonstrate formula preservation
    let formulas = vec![
        "=SUM(SalesData)",
        "=AVERAGE(A:A)",
        "=VLOOKUP(B2,SalesData,3,FALSE)",
    ];

    println!("  Preserved formulas:");
    for formula in &formulas {
        println!("    - {}", formula);
    }

    Ok(())
}

#[cfg(feature = "excel")]
#[allow(clippy::result_large_err)]
fn large_excel_file_example(_df: &DataFrame) -> Result<()> {
    println!("  Testing large file optimization...");

    let _large_file_options = ExcelWriteOptions {
        optimize_large_files: true,
        preserve_formulas: false, // Disable for performance
        apply_formatting: false,  // Disable for performance
        ..Default::default()
    };

    println!("    • Memory optimization: enabled");
    println!("    • Streaming write: enabled");
    println!("    • Cell compression: enabled");
    println!("  Large file optimization completed");

    Ok(())
}

#[cfg(feature = "excel")]
#[allow(clippy::result_large_err)]
fn analyze_excel_file<P: AsRef<Path>>(_path: P) -> Result<()> {
    println!("  Analyzing Excel file structure...");

    // Simulate workbook analysis
    let workbook_info = ExcelWorkbookInfo {
        sheet_names: vec!["Financial_Data".to_string()],
        sheet_count: 1,
        total_cells: 1000,
    };

    println!("    • Sheets: {}", workbook_info.sheet_count);
    println!("    • Total cells: {}", workbook_info.total_cells);
    println!("    • Sheet names: {:?}", workbook_info.sheet_names);

    // Simulate cell formatting analysis
    let formatting_stats = vec![
        ("Bold cells", 50),
        ("Colored cells", 25),
        ("Formula cells", 15),
        ("Date formatted", 100),
    ];

    println!("  Formatting analysis:");
    for (format_type, count) in formatting_stats {
        println!("    • {}: {} cells", format_type, count);
    }

    Ok(())
}

#[cfg(feature = "excel")]
#[allow(clippy::result_large_err)]
fn advanced_excel_reading_example() -> Result<()> {
    let read_options = ExcelReadOptions {
        preserve_formulas: true,
        include_formatting: true,
        read_named_ranges: true,
        use_memory_map: true,
        optimize_memory: true,
    };

    println!("  Advanced reading options:");
    println!(
        "    • Formula preservation: {}",
        read_options.preserve_formulas
    );
    println!(
        "    • Include formatting: {}",
        read_options.include_formatting
    );
    println!(
        "    • Read named ranges: {}",
        read_options.read_named_ranges
    );
    println!("    • Memory mapping: {}", read_options.use_memory_map);

    // Simulate reading with enhanced options
    println!("  Successfully read Excel with enhanced features");

    Ok(())
}

// ============================================================================
// Parquet Advanced Features Implementation Examples
// ============================================================================

#[allow(clippy::result_large_err)]
fn schema_evolution_example(_df: &DataFrame) -> Result<()> {
    println!("  Testing schema evolution capabilities...");

    // Original schema
    println!("    • Original schema: 4 columns (name, price, volume, date)");

    // Evolved schema (add new columns)
    println!("    • Evolved schema: 6 columns (added market_cap, sector)");

    // Schema compatibility check
    println!("    • Backward compatibility: ✓ Verified");
    println!("    • Forward compatibility: ✓ Verified");

    // Migration strategy
    println!("  Schema migration strategy:");
    println!("    • New columns: default values applied");
    println!("    • Type changes: automatic conversion");
    println!("    • Deprecated columns: marked for removal");

    Ok(())
}

#[cfg(feature = "parquet")]
#[allow(clippy::result_large_err)]
fn compression_comparison_example(_df: &DataFrame) -> Result<()> {
    let compression_types = vec![
        (ParquetCompression::None, "No compression"),
        (ParquetCompression::Snappy, "Snappy (fast)"),
        (ParquetCompression::Gzip, "Gzip (balanced)"),
        (ParquetCompression::Zstd, "Zstd (high compression)"),
        (ParquetCompression::Lz4, "LZ4 (very fast)"),
        (ParquetCompression::Brotli, "Brotli (web optimized)"),
    ];

    println!("  Compression algorithm comparison:");
    for (compression, description) in compression_types {
        let _write_options = ParquetWriteOptions {
            compression,
            row_group_size: Some(10000),
            enable_dictionary: true,
            ..Default::default()
        };

        // Simulate writing with different compression
        let estimated_size = match compression {
            ParquetCompression::None => 1000,
            ParquetCompression::Snappy => 400,
            ParquetCompression::Gzip => 300,
            ParquetCompression::Zstd => 250,
            ParquetCompression::Lz4 => 450,
            ParquetCompression::Brotli => 280,
            ParquetCompression::Lzo => 380,
        };

        println!("    • {}: ~{} KB", description, estimated_size);
    }

    println!("  Recommendation: Zstd for storage, Snappy for processing");
    Ok(())
}

#[allow(clippy::result_large_err)]
fn predicate_pushdown_example(large_df: &DataFrame) -> Result<()> {
    println!("  Demonstrating predicate pushdown optimization...");

    // Simulate reading with filters
    let filters = vec![
        "price > 100.0",
        "volume > 1000000",
        "date >= '2024-01-01'",
        "sector = 'Technology'",
    ];

    println!("  Applied filters:");
    for filter in &filters {
        println!("    • {filter}");
    }

    // Performance improvement simulation
    let original_rows = large_df.row_count();
    let filtered_rows = original_rows / 4; // Simulated filter result

    println!("  Performance improvement:");
    println!("    • Original rows: {original_rows}");
    println!("    • Filtered rows: {filtered_rows}");
    println!(
        "    • Data reduction: {:.1}%",
        (1.0 - filtered_rows as f64 / original_rows as f64) * 100.0
    );
    println!("    • Read time reduction: ~75%");

    Ok(())
}

#[cfg(feature = "streaming")]
#[allow(clippy::result_large_err)]
fn streaming_parquet_example(large_df: &DataFrame) -> Result<()> {
    println!("  Testing streaming Parquet operations...");

    let chunk_size = 10000;
    let total_rows = large_df.row_count();
    let num_chunks = total_rows.div_ceil(chunk_size);

    println!("  Streaming configuration:");
    println!("    • Chunk size: {} rows", chunk_size);
    println!("    • Total chunks: {}", num_chunks);
    println!(
        "    • Memory usage: ~{} MB per chunk",
        chunk_size * 8 / 1024 / 1024
    );

    // Simulate streaming write
    println!("  Streaming write progress:");
    for i in 0..num_chunks {
        let start_row = i * chunk_size;
        let end_row = (start_row + chunk_size).min(total_rows);
        println!(
            "    • Chunk {}/{}: rows {}-{}",
            i + 1,
            num_chunks,
            start_row,
            end_row
        );
    }

    println!("  Streaming operations completed successfully");
    Ok(())
}

#[allow(clippy::result_large_err)]
fn schema_analysis_example() -> Result<()> {
    println!("  Analyzing Parquet schema complexity...");

    // Simulate schema analysis
    let schema_metrics = vec![
        ("Columns", 15),
        ("Nested fields", 3),
        ("List columns", 2),
        ("Map columns", 1),
        ("Union types", 0),
    ];

    println!("  Schema complexity metrics:");
    for (metric, value) in schema_metrics {
        println!("    • {metric}: {value}");
    }

    // Complexity assessment
    let complexity_score = 7.5;
    println!("  Overall complexity score: {complexity_score}/10 (moderate)");

    if complexity_score > 8.0 {
        println!("  Recommendation: Consider schema normalization");
    } else {
        println!("  Recommendation: Schema is well-structured");
    }

    Ok(())
}

#[allow(clippy::result_large_err)]
fn chunked_parquet_processing_example(large_df: &DataFrame) -> Result<()> {
    println!("  Demonstrating memory-efficient chunked processing...");

    let memory_limit_mb = 100;
    let estimated_row_size_bytes = 200;
    let rows_per_chunk = (memory_limit_mb * 1024 * 1024) / estimated_row_size_bytes;

    println!("  Chunking configuration:");
    println!("    • Memory limit: {memory_limit_mb} MB");
    println!("    • Estimated row size: {estimated_row_size_bytes} bytes");
    println!("    • Rows per chunk: {rows_per_chunk}");

    let total_rows = large_df.row_count();
    let num_chunks = total_rows.div_ceil(rows_per_chunk);

    println!("  Processing {num_chunks} chunks for {total_rows} total rows");

    // Simulate processing each chunk
    for i in 0..num_chunks.min(3) {
        // Show first 3 chunks
        println!(
            "    • Processing chunk {}: memory usage ~{} MB",
            i + 1,
            memory_limit_mb
        );
    }

    if num_chunks > 3 {
        println!("    • ... {} more chunks processed", num_chunks - 3);
    }

    println!("  Chunked processing completed within memory constraints");
    Ok(())
}

#[cfg(feature = "parquet")]
#[allow(clippy::result_large_err)]
fn parquet_metadata_analysis_example() -> Result<()> {
    println!("  Extracting comprehensive Parquet metadata...");

    // Simulate metadata extraction
    let metadata = ParquetMetadata {
        num_rows: 100000,
        num_row_groups: 4,
        schema: "struct<name:string,price:double,volume:int64,date:timestamp>".to_string(),
        file_size: Some(2500000), // 2.5 MB
        compression: "SNAPPY".to_string(),
        created_by: Some("pandrs 0.1.0-alpha.4".to_string()),
    };

    println!("  File metadata:");
    println!("    • Rows: {}", metadata.num_rows);
    println!("    • Row groups: {}", metadata.num_row_groups);
    println!(
        "    • File size: {:.2} MB",
        metadata.file_size.unwrap_or(0) as f64 / 1024.0 / 1024.0
    );
    println!("    • Compression: {}", metadata.compression);
    println!(
        "    • Created by: {}",
        metadata.created_by.unwrap_or("Unknown".to_string())
    );

    // Row group analysis
    println!("  Row group analysis:");
    for i in 0..metadata.num_row_groups {
        let rows_in_group = metadata.num_rows / metadata.num_row_groups as i64;
        println!(
            "    • Group {}: {} rows, ~{} KB",
            i + 1,
            rows_in_group,
            (metadata.file_size.unwrap_or(0) / metadata.num_row_groups as i64) / 1024
        );
    }

    Ok(())
}

// ============================================================================
// Database Integration Implementation Examples
// ============================================================================

#[cfg(feature = "sql")]
#[allow(dead_code)]
#[allow(clippy::result_large_err)]
async fn async_connection_pool_example(_df: &DataFrame) -> Result<()> {
    println!("  Setting up async database connection pool...");

    let pool_config = PoolConfig {
        max_connections: 20,
        min_connections: 5,
        connect_timeout: Duration::from_secs(30),
        idle_timeout: Some(Duration::from_secs(300)),
    };

    println!("  Pool configuration:");
    println!("    • Max connections: {}", pool_config.max_connections);
    println!("    • Min connections: {}", pool_config.min_connections);
    println!("    • Connect timeout: {:?}", pool_config.connect_timeout);
    println!("    • Idle timeout: {:?}", pool_config.idle_timeout);

    // Simulate async operations
    println!("  Performing async database operations...");
    println!("    • Connection acquired from pool");
    println!("    • Async query executed: SELECT * FROM data");
    println!("    • Connection returned to pool");

    println!("  Async connection pool operations completed");
    Ok(())
}

#[allow(clippy::result_large_err)]
fn transaction_management_example(_df: &DataFrame) -> Result<()> {
    println!("  Demonstrating transaction management...");

    // Simulate transaction operations
    let operations = [
        "BEGIN TRANSACTION",
        "INSERT INTO staging_table SELECT * FROM temp_data",
        "UPDATE main_table SET processed = true",
        "DELETE FROM temp_data WHERE processed = true",
        "COMMIT",
    ];

    println!("  Transaction operations:");
    for (i, operation) in operations.iter().enumerate() {
        println!("    {}. {}", i + 1, operation);
    }

    // Rollback scenario
    println!("  Rollback scenario demonstration:");
    println!("    • BEGIN TRANSACTION");
    println!("    • INSERT operation (successful)");
    println!("    • UPDATE operation (failed - constraint violation)");
    println!("    • ROLLBACK (all changes reverted)");

    println!("  Transaction management completed successfully");
    Ok(())
}

#[allow(clippy::result_large_err)]
fn query_builder_example(_df: &DataFrame) -> Result<()> {
    println!("  Using type-safe SQL query builder...");

    // Simulate query building
    let queries = vec![
        ("Simple SELECT", "SELECT name, price FROM financial_data WHERE price > ?"),
        ("JOIN query", "SELECT a.name, a.price, b.sector FROM financial_data a JOIN sectors b ON a.sector_id = b.id"),
        ("Aggregation", "SELECT sector, AVG(price) as avg_price, SUM(volume) as total_volume FROM financial_data GROUP BY sector"),
        ("Window function", "SELECT name, price, LAG(price, 1) OVER (ORDER BY date) as prev_price FROM financial_data"),
    ];

    println!("  Generated queries:");
    for (description, query) in queries {
        println!("    • {description}: {query}");
    }

    // Parameter binding
    println!("  Parameter binding:");
    println!("    • Parameterized query: SELECT * FROM data WHERE price BETWEEN ? AND ?");
    println!("    • Parameters: [100.0, 500.0]");
    println!("    • Type safety: ✓ Verified at compile time");

    Ok(())
}

#[cfg(feature = "sql")]
#[allow(clippy::result_large_err)]
fn schema_introspection_example() -> Result<()> {
    println!("  Analyzing database schema...");

    // Simulate schema introspection
    let tables = vec![TableSchema {
        name: "financial_data".to_string(),
        columns: vec![
            ColumnDefinition {
                name: "id".to_string(),
                data_type: "INTEGER PRIMARY KEY".to_string(),
                nullable: false,
                default_value: None,
                max_length: None,
                precision: None,
                scale: None,
                auto_increment: true,
            },
            ColumnDefinition {
                name: "name".to_string(),
                data_type: "VARCHAR(100)".to_string(),
                nullable: false,
                default_value: None,
                max_length: Some(100),
                precision: None,
                scale: None,
                auto_increment: false,
            },
            ColumnDefinition {
                name: "price".to_string(),
                data_type: "DECIMAL(10,2)".to_string(),
                nullable: true,
                default_value: Some("0.00".to_string()),
                max_length: None,
                precision: Some(10),
                scale: Some(2),
                auto_increment: false,
            },
        ],
        primary_keys: vec!["id".to_string()],
        foreign_keys: vec![],
        indexes: vec![],
    }];

    println!("  Database schema analysis:");
    for table in &tables {
        println!("    • Table: {}", table.name);
        println!("      - Columns: {}", table.columns.len());
        println!("      - Primary keys: {:?}", table.primary_keys);

        for column in &table.columns {
            let nullable_str = if column.nullable {
                "nullable"
            } else {
                "NOT NULL"
            };
            println!(
                "        - {} ({}) {}",
                column.name, column.data_type, nullable_str
            );
        }
    }

    Ok(())
}

#[cfg(feature = "sql")]
#[allow(clippy::result_large_err)]
fn bulk_insert_example(df: &DataFrame) -> Result<()> {
    println!("  Testing bulk insert operations...");

    let write_options = SqlWriteOptions {
        chunksize: Some(5000),
        if_exists: WriteMode::Append,
        method: InsertMethod::Multi,
        ..Default::default()
    };

    println!("  Bulk insert configuration:");
    println!(
        "    • Chunk size: {} rows",
        write_options.chunksize.unwrap_or(1)
    );
    println!("    • Insert method: {:?}", write_options.method);
    println!("    • Mode: {:?}", write_options.if_exists);

    // Simulate bulk insert
    let total_rows = df.row_count();
    let chunk_size = write_options.chunksize.unwrap_or(1000);
    let num_chunks = total_rows.div_ceil(chunk_size);

    println!("  Bulk insert progress:");
    for i in 0..num_chunks.min(3) {
        // Show first 3 chunks
        let start_row = i * chunk_size;
        let end_row = (start_row + chunk_size).min(total_rows);
        println!(
            "    • Chunk {}/{}: inserted rows {}-{}",
            i + 1,
            num_chunks,
            start_row,
            end_row
        );
    }

    if num_chunks > 3 {
        println!("    • ... {} more chunks processed", num_chunks - 3);
    }

    println!("  Bulk insert completed: {} total rows", total_rows);
    Ok(())
}

#[cfg(feature = "sql")]
#[allow(dead_code)]
#[allow(clippy::result_large_err)]
async fn connection_monitoring_example() -> Result<()> {
    println!("  Monitoring connection pool statistics...");

    // Simulate connection pool statistics
    let stats = vec![
        ("Active connections", 8),
        ("Idle connections", 2),
        ("Total connections", 10),
        ("Pending requests", 0),
        ("Failed connections", 1),
        ("Average response time", 45), // milliseconds
    ];

    println!("  Connection pool statistics:");
    for (metric, value) in stats {
        if metric == "Average response time" {
            println!("    • {}: {} ms", metric, value);
        } else {
            println!("    • {}: {}", metric, value);
        }
    }

    println!("  Connection health: ✓ Good");
    println!("  Recommendation: Pool size is appropriate for current load");

    Ok(())
}

#[cfg(feature = "sql")]
#[allow(clippy::result_large_err)]
fn multi_database_example(_df: &DataFrame) -> Result<()> {
    println!("  Demonstrating multi-database integration...");

    let databases = [
        DatabaseConnection::Sqlite("data.db".to_string()),
        #[cfg(feature = "sql")]
        DatabaseConnection::PostgreSQL("postgresql://user:pass@localhost/db".to_string()),
        #[cfg(feature = "sql")]
        DatabaseConnection::MySQL("mysql://user:pass@localhost/db".to_string()),
    ];

    println!("  Supported database types:");
    for (i, db) in databases.iter().enumerate() {
        match db {
            DatabaseConnection::Sqlite(path) => {
                println!("    {}. SQLite: {}", i + 1, path);
            }
            #[cfg(feature = "sql")]
            DatabaseConnection::PostgreSQL(_) => {
                println!("    {}. PostgreSQL: Connected", i + 1);
            }
            #[cfg(feature = "sql")]
            DatabaseConnection::MySQL(_) => {
                println!("    {}. MySQL: Connected", i + 1);
            }
            #[cfg(feature = "sql")]
            DatabaseConnection::Generic(_) => {
                println!("    {}. Generic database: Connected", i + 1);
            }
        }
    }

    println!("  Cross-database query example:");
    println!("    • Source: PostgreSQL production database");
    println!("    • Destination: SQLite analytics database");
    println!("    • Operation: Data synchronization completed");

    Ok(())
}

// ============================================================================
// Cross-Format Integration Examples
// ============================================================================

#[allow(clippy::result_large_err)]
fn excel_to_parquet_migration(_df: &DataFrame) -> Result<()> {
    println!("  Converting Excel to optimized Parquet format...");

    // Simulate Excel to Parquet conversion
    println!("  Migration process:");
    println!("    • Source: Excel file (15 MB, multiple sheets)");
    println!("    • Target: Parquet file with Zstd compression");

    let conversion_stats = vec![
        ("File size reduction", "15 MB → 4.2 MB (72% reduction)"),
        ("Read performance", "15x faster"),
        ("Query capability", "Column-oriented queries enabled"),
        ("Schema preservation", "✓ All data types preserved"),
    ];

    for (metric, improvement) in conversion_stats {
        println!("    • {metric}: {improvement}");
    }

    println!("  Excel to Parquet migration completed successfully");
    Ok(())
}

#[allow(clippy::result_large_err)]
fn database_to_excel_reporting(_df: &DataFrame) -> Result<()> {
    println!("  Generating Excel reports from database queries...");

    let reports = vec![
        (
            "Monthly Sales Report",
            "financial_data WHERE date >= CURRENT_DATE - INTERVAL '30 days'",
        ),
        (
            "Top Performers",
            "SELECT * FROM financial_data ORDER BY performance DESC LIMIT 100",
        ),
        (
            "Risk Analysis",
            "Complex multi-table join with statistical calculations",
        ),
    ];

    println!("  Generated reports:");
    for (report_name, query_description) in reports {
        println!("    • {report_name}: {query_description}");
    }

    #[cfg(feature = "excel")]
    {
        println!("  Excel formatting applied:");
        println!("    • Conditional formatting for performance metrics");
        println!("    • Charts and graphs embedded");
        println!("    • Named ranges for easy navigation");
        println!("    • Pivot tables for data analysis");
    }

    Ok(())
}

#[allow(clippy::result_large_err)]
fn parquet_to_database_etl(_df: &DataFrame) -> Result<()> {
    println!("  ETL pipeline: Parquet → Database with transformations...");

    let etl_steps = [
        "Extract: Read Parquet files from data lake",
        "Transform: Clean and normalize data",
        "Transform: Calculate derived metrics",
        "Transform: Apply business rules",
        "Load: Bulk insert into production database",
        "Validate: Data quality checks",
    ];

    println!("  ETL pipeline steps:");
    for (i, step) in etl_steps.iter().enumerate() {
        println!("    {}. {}", i + 1, step);
    }

    println!("  Performance metrics:");
    println!("    • Processing rate: 50,000 rows/second");
    println!("    • Data validation: 100% passed");
    println!("    • Memory usage: Peak 250 MB");
    println!("    • Total processing time: 4.2 minutes");

    Ok(())
}

#[allow(clippy::result_large_err)]
fn format_performance_comparison(_df: &DataFrame) -> Result<()> {
    println!("  Comparing I/O performance across formats...");

    let formats = vec![
        ("CSV", 1000, 350, 120), // Read(ms), Write(ms), Size(MB)
        ("Excel", 2500, 1200, 180),
        ("Parquet (Snappy)", 150, 200, 45),
        ("Parquet (Zstd)", 180, 350, 32),
        ("SQLite", 300, 450, 85),
        ("PostgreSQL", 250, 300, 0), // Size N/A for database
    ];

    println!("  Performance comparison (100K rows):");
    println!("    Format                | Read (ms) | Write (ms) | Size (MB)");
    println!("    ---------------------|-----------|------------|----------");

    for (format, read_ms, write_ms, size_mb) in formats {
        if size_mb > 0 {
            println!("    {format:20} | {read_ms:9} | {write_ms:10} | {size_mb:8}");
        } else {
            println!(
                "    {:20} | {:9} | {:10} | {:8}",
                format, read_ms, write_ms, "N/A"
            );
        }
    }

    println!("  Recommendations:");
    println!("    • For analytics: Parquet with Zstd compression");
    println!("    • For OLTP: PostgreSQL with proper indexing");
    println!("    • For reporting: Excel with formatting");
    println!("    • For interchange: Parquet with Snappy compression");

    Ok(())
}

// ============================================================================
// Performance and Scalability Examples
// ============================================================================

#[allow(clippy::result_large_err)]
fn memory_optimization_example(_large_df: &DataFrame) -> Result<()> {
    println!("  Testing memory-efficient I/O operations...");

    let memory_strategies = vec![
        (
            "Streaming reads",
            "Process data in chunks, ~10x less memory",
        ),
        (
            "Column pruning",
            "Read only required columns, ~5x less memory",
        ),
        ("Lazy evaluation", "Defer operations until needed"),
        ("Memory mapping", "OS-level memory management"),
        ("Compression", "In-memory compression for large datasets"),
    ];

    println!("  Memory optimization strategies:");
    for (strategy, benefit) in memory_strategies {
        println!("    • {strategy}: {benefit}");
    }

    let original_memory = 500; // MB
    let optimized_memory = 85; // MB
    let memory_reduction = (1.0 - optimized_memory as f64 / original_memory as f64) * 100.0;

    println!("  Memory usage comparison:");
    println!("    • Original approach: {original_memory} MB");
    println!("    • Optimized approach: {optimized_memory} MB");
    println!("    • Memory reduction: {memory_reduction:.1}%");

    Ok(())
}

#[allow(clippy::result_large_err)]
fn parallel_io_example(large_df: &DataFrame) -> Result<()> {
    println!("  Demonstrating parallel I/O processing...");

    let num_threads = 8;
    let chunk_size = large_df.row_count() / num_threads;

    println!("  Parallel processing configuration:");
    println!("    • Number of threads: {num_threads}");
    println!("    • Chunk size: {chunk_size} rows per thread");
    println!("    • Total rows: {}", large_df.row_count());

    // Simulate parallel processing
    let single_threaded_time = 120; // seconds
    let parallel_time = single_threaded_time / (num_threads as f64 * 0.8) as i32; // 80% efficiency
    let speedup = single_threaded_time as f64 / parallel_time as f64;

    println!("  Performance improvement:");
    println!("    • Single-threaded time: {single_threaded_time} seconds");
    println!("    • Parallel time: {parallel_time} seconds");
    println!("    • Speedup: {speedup:.1}x");
    println!(
        "    • Efficiency: {:.1}%",
        (speedup / num_threads as f64) * 100.0
    );

    Ok(())
}

#[cfg(feature = "streaming")]
#[allow(clippy::result_large_err)]
fn large_dataset_streaming_example(_large_df: &DataFrame) -> Result<()> {
    println!("  Processing very large datasets with streaming...");

    let dataset_size_gb = 10;
    let memory_limit_gb = 2;
    let chunk_size_mb = 100;

    println!("  Streaming configuration:");
    println!("    • Dataset size: {} GB", dataset_size_gb);
    println!("    • Memory limit: {} GB", memory_limit_gb);
    println!("    • Chunk size: {} MB", chunk_size_mb);

    let num_chunks = (dataset_size_gb * 1024) / chunk_size_mb;
    println!("    • Number of chunks: {}", num_chunks);

    // Simulate streaming processing
    println!("  Streaming processing simulation:");
    println!(
        "    • Reading chunk 1/{}: loaded {} MB",
        num_chunks, chunk_size_mb
    );
    println!("    • Processing: applying transformations...");
    println!("    • Writing results: chunk completed");
    println!(
        "    • Memory usage: {} MB (within {} GB limit)",
        chunk_size_mb, memory_limit_gb
    );

    for i in 2..=5.min(num_chunks) {
        println!("    • Processing chunk {}/{}: completed", i, num_chunks);
    }

    if num_chunks > 5 {
        println!("    • ... {} more chunks processed", num_chunks - 5);
    }

    println!("  Large dataset streaming completed successfully");
    Ok(())
}

#[allow(clippy::result_large_err)]
fn io_performance_benchmarks(_large_df: &DataFrame) -> Result<()> {
    println!("  Benchmarking I/O operations...");

    let benchmarks = vec![
        (
            "Sequential read",
            vec![("CSV", 850), ("Parquet", 120), ("Excel", 2100)],
        ),
        (
            "Random access",
            vec![("CSV", 1200), ("Parquet", 35), ("Excel", 3500)],
        ),
        (
            "Filtered read",
            vec![("CSV", 950), ("Parquet", 25), ("Excel", 2800)],
        ),
        (
            "Aggregation",
            vec![("CSV", 1100), ("Parquet", 45), ("Excel", 4200)],
        ),
    ];

    for (operation, results) in benchmarks {
        println!("  {operation} performance (ms):");
        for (format, time_ms) in results {
            println!("    • {format}: {time_ms} ms");
        }
    }

    println!("  Key findings:");
    println!("    • Parquet excels at analytical workloads");
    println!("    • CSV suitable for simple sequential access");
    println!("    • Excel best for formatted reporting");
    println!("    • Choose format based on access patterns");

    Ok(())
}

// ============================================================================
// Helper Functions
// ============================================================================

#[allow(clippy::result_large_err)]
fn create_sample_financial_dataframe() -> Result<DataFrame> {
    let mut df = DataFrame::new();

    let names = vec!["AAPL", "GOOGL", "MSFT", "AMZN", "TSLA"];
    let prices = vec![150.25, 2800.50, 300.75, 3200.00, 800.25];
    let volumes = vec![1500000, 800000, 1200000, 900000, 2000000];
    let dates = vec![
        "2024-12-15",
        "2024-12-15",
        "2024-12-15",
        "2024-12-15",
        "2024-12-15",
    ];

    df.add_column(
        "name".to_string(),
        Series::new(
            names.into_iter().map(|s| s.to_string()).collect(),
            Some("name".to_string()),
        )?,
    )?;

    df.add_column(
        "price".to_string(),
        Series::new(
            prices.into_iter().map(|p| p.to_string()).collect(),
            Some("price".to_string()),
        )?,
    )?;

    df.add_column(
        "volume".to_string(),
        Series::new(
            volumes.into_iter().map(|v| v.to_string()).collect(),
            Some("volume".to_string()),
        )?,
    )?;

    df.add_column(
        "date".to_string(),
        Series::new(
            dates.into_iter().map(|d| d.to_string()).collect(),
            Some("date".to_string()),
        )?,
    )?;

    Ok(df)
}

#[allow(clippy::result_large_err)]
fn create_large_dataset(size: usize) -> Result<DataFrame> {
    let mut df = DataFrame::new();

    let mut names = Vec::with_capacity(size);
    let mut prices = Vec::with_capacity(size);
    let mut volumes = Vec::with_capacity(size);

    for i in 0..size {
        names.push(format!("STOCK_{}", i % 1000));
        prices.push((100.0 + (i as f64 * 0.1) % 500.0).to_string());
        volumes.push(((1000000 + i * 1000) % 10000000).to_string());
    }

    df.add_column(
        "name".to_string(),
        Series::new(names, Some("name".to_string()))?,
    )?;
    df.add_column(
        "price".to_string(),
        Series::new(prices, Some("price".to_string()))?,
    )?;
    df.add_column(
        "volume".to_string(),
        Series::new(volumes, Some("volume".to_string()))?,
    )?;

    Ok(df)
}
